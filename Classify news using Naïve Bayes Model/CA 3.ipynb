{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify news using Naïve Bayes Model\n",
    "In this computer assignment, we implement a Naïve Bayes model to find the classify news using their descriptions and headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "1. `re` is imported to remove non-alphabetic characters from strings.\n",
    "\n",
    "2. `nlkt` is imported to preprocess given descriptions and headlines such as removing stop words and stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install nltk\n",
    "import re\n",
    "import nltk\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from IPython.display import Markdown, display\n",
    "# nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "There is a `data.csv` file including a list of news with some details such as authors, category, headline, date and short_description.Also, there is a `test.csv` file as well as `data.csv` except it doesn't contain category field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Interference\n",
    "Naïve Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (naïve) independence assumptions between the features. One common rule is to pick the hypothesis that is most probable; this is known as the maximum a posteriori or MAP decision rule. \n",
    "$$ C_{MAP} = argmax \\ P(c|d) = argmax \\ \\frac{P(d|c)P(c)}{P(d)} $$\n",
    ">### 1.Posterior probability $P(c|d)$:\n",
    "$$P(c|d) = \\frac{P(d|c) P(c)}{P(d)}$$\n",
    "\n",
    ">### 2.Class prior probability $P(c)$ :\n",
    "$$P(c) = \\frac{count(c)}{\\sum_{c_i}count(c_i)}$$\n",
    "\n",
    ">### 3.Likelihood $P(d|c)$:\n",
    "$$P(d|c) = P(w_1|c)P(w_2|c)...P(w_n|c) = \\prod_{i=1,..,n}\\frac{count(w_i, c) + 1}{\\sum_{w \\in V} count(w, c) + |V|}$$\n",
    "\n",
    "Using above equations we can pich the most probable class.\n",
    "$$\\Rightarrow C_{MAP} = argmax \\ P(d|c)P(c) = argmax \\ P(w_1|c)P(w_2|c)...P(w_n|c)P(c) = argmax \\prod_{i=1,..,n}\\frac{count(w_i, c) + 1}{\\sum_{w \\in V} count(w, c) + |V|} P(c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling\n",
    "\n",
    "Imbalanced dataset may lead to a difference between value of recall and precision. A method to tackle this issue is oversampling. To do this, after finding the maximum size of classes, the number of data of other classes should be should increased. In other words, oversampling consists in replicating some points from the minority class in order to increase its cardinality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Classifier\n",
    "`Bayesian_Classifier()` is in charge of cleaning and training a Bayesian model to classify news using their short_descriptions and headlines.\n",
    "\n",
    "### 1. `clean_data()`\n",
    ">This method removes all non-alphabetic characters from given field of a dataframe and stop words which are frequent words in English using nltk. It also producing morphological variants of a root/base word based on given method, stemming or lemmatization.\n",
    "\n",
    "### 2. `over_sample()`\n",
    ">In order to handle imbalanced dataset using oversampling method, it's necessary to increase the data size of each category to size of the biggest one. The duplicated data are selected randomly from the data using `DataFrame.sample()`.\n",
    "\n",
    "### 3. `split_train_validation()`\n",
    ">It split dataframe into a validation dataframe and train dataframe with $splitRatio = 0.8$ . If the `oversample` flag is set to ture, it calls `over_sample()` to handle our imblanced dataset.\n",
    "\n",
    "### 4. `prepare_dict()`\n",
    ">In this method, a nested dictionary is created that store the number of usage each word in every category.\n",
    "\n",
    "### 5. `preprocessing()`\n",
    ">Removing NaN values, cleaning data, spliting them into train and validation, and preparing dict are the required preprocessing steps, implemented in this function. \n",
    "\n",
    "### 6. `calculate_confusion_matrix()`\n",
    ">It prepare a numpy array from a dictionary which contains the prediction results of each category. \n",
    "\n",
    "### 7. `find_categories()`\n",
    ">It gives a dataframe and return predicted labels according to bayesian interference in a list.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     2,
     5,
     24,
     36,
     50,
     64,
     84,
     96,
     105,
     108,
     111,
     117,
     159,
     170
    ]
   },
   "outputs": [],
   "source": [
    "class Bayesian_Classifier:\n",
    "        \n",
    "    def over_sample(self, data, final_size):\n",
    "        return data.append(data.sample(n = final_size - len(data)), ignore_index = True) \n",
    "        \n",
    "    def split_train_validation(self, data, categories, oversample):\n",
    "        max_size = data['category'].value_counts().max()*4//5\n",
    "        categorized_data = data.groupby(['category'])\n",
    "        train_data = []\n",
    "        validation_data = []\n",
    "        category_names = []\n",
    "        for category, dataframe in categorized_data:\n",
    "            if (categories) and (not category in categories):\n",
    "                continue\n",
    "            train_dataframe = dataframe[:len(dataframe)*4//5]  \n",
    "            if oversample:\n",
    "                train_dataframe = self.over_sample(train_dataframe, max_size)\n",
    "            train_data.append(train_dataframe)\n",
    "            validation_data.append(dataframe[len(dataframe)*4//5:])\n",
    "            category_names.append(category)    \n",
    "        number_of_data_in_each_class = {category_names[i]: len(train_data[i]) for i in range(len(train_data))}\n",
    "            \n",
    "        return pd.concat(train_data), pd.concat(validation_data), category_names, number_of_data_in_each_class\n",
    "\n",
    "    def clean_data(self, data, field, method):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        for _, row in data.iterrows():\n",
    "            s = re.sub('[^0-9a-zA-Z]+', ' ',  row[field]).lower()\n",
    "            word_list = []\n",
    "            for w in nltk.word_tokenize(s):\n",
    "                if w in stop_words:\n",
    "                    continue\n",
    "                word_list.append(method(w))\n",
    "            row[field] = ' '.join(word_list)\n",
    "        return data\n",
    "    \n",
    "    def prepare_dict(self, train_data, category_names, field):\n",
    "        all_words = set()\n",
    "        training_dict = {c : dict() for c in category_names}    \n",
    "        for _, row in train_data.iterrows():\n",
    "            category = row['category']\n",
    "            for word in row[field].split():\n",
    "                all_words.add(word)\n",
    "                if word in training_dict[category]:\n",
    "                    training_dict[category][word] += 1\n",
    "                else:\n",
    "                    training_dict[category][word] = 1\n",
    "                    \n",
    "        return training_dict, all_words\n",
    "    \n",
    "    def preprocessing(self, data, method, categories, oversample):\n",
    "        data = data.replace(np.nan, '', regex=True)\n",
    "        data = pd.DataFrame(data = {'category' : data.loc[:, 'category'], \\\n",
    "                                    'description' : data.loc[:, 'short_description'] + \" \" + data.loc[:, 'headline'] + \" \" + data.loc[:, 'headline']})\n",
    "        \n",
    "        data = data.dropna().reset_index(drop=True).sample(frac = 1)\n",
    "        data = self.clean_data(data, 'description', method)\n",
    "        train_data, validation_data, category_names, number_of_data_in_each_class = self.split_train_validation(data, categories, oversample)\n",
    "        \n",
    "        \n",
    "        training_dict, all_words = self.prepare_dict(train_data, category_names, 'description')\n",
    "        \n",
    "        return training_dict, validation_data, category_names, len(all_words), number_of_data_in_each_class\n",
    "\n",
    "    def __init__(self, train_file = \"data.csv\", method=\"lemmatize\", categories=[], oversample=True):\n",
    "        if method == \"lemmatize\":\n",
    "            self.method = WordNetLemmatizer().lemmatize\n",
    "        elif method == \"stem\":    \n",
    "            self.method = PorterStemmer().stem\n",
    "        \n",
    "        data = pd.read_csv(train_file)\n",
    "        training_dict, validation_data, category_names, num_of_all_words, number_of_data_in_each_class = self.preprocessing(data, self.method, categories, oversample)\n",
    "\n",
    "        self.num_of_all_words = num_of_all_words\n",
    "        self.training_dict = training_dict\n",
    "        self.category_names = category_names\n",
    "        self.validation_data = validation_data\n",
    "        self.num_of_words_each_class = {c : len(training_dict[c]) for c in training_dict}\n",
    "        \n",
    "        total_num_of_data = 0\n",
    "        for c in category_names:\n",
    "            total_num_of_data += number_of_data_in_each_class[c]\n",
    "        self.probability_of_each_class = {c : number_of_data_in_each_class[c]/total_num_of_data for c in category_names}\n",
    "        \n",
    "    def find_categories(self, dataframe, field):\n",
    "        result = []\n",
    "        for _, row in dataframe.iterrows():\n",
    "            p = {c : self.probability_of_each_class[c] for c in self.category_names}\n",
    "            for c in self.category_names:\n",
    "                for word in row[field].split():\n",
    "                    if word in self.training_dict[c]: \n",
    "                        p[c] *= (self.training_dict[c][word] + 1)\n",
    "                    p[c] *= 5000/(self.num_of_words_each_class[c] + self.num_of_all_words)\n",
    "            result.append(max(p.items(), key=operator.itemgetter(1))[0])\n",
    "        return result\n",
    "    \n",
    "    def calculate_confusion_matrix(self, result, category_names):\n",
    "        num_of_classes = len(self.category_names)\n",
    "        confusion_matrix = np.zeros((num_of_classes, num_of_classes), dtype=int)\n",
    "        for index1, category1 in enumerate(category_names):\n",
    "            for index2, category2 in enumerate(category_names):\n",
    "                confusion_matrix[index1][index2] = result[category1][category2]\n",
    "            \n",
    "        return confusion_matrix\n",
    "        \n",
    "    def calculate_recall(self, confusion_matrix, index):\n",
    "        return confusion_matrix[index, index]/np.sum(confusion_matrix[index])\n",
    "    \n",
    "    def calculate_precision(self, confusion_matrix, index):\n",
    "        return confusion_matrix[index, index]/np.sum(confusion_matrix[:, index])\n",
    "    \n",
    "    def calculate_accuracy(self, confusion_matrix):\n",
    "        acc = 0\n",
    "        for i in range(len(confusion_matrix)):\n",
    "            acc += confusion_matrix[i, i]\n",
    "        return acc/np.sum(confusion_matrix)\n",
    "    \n",
    "    def show_confusion_matrix_and_evaluation_measures_table(self, confusion_matrix):\n",
    "        table = \"<center>\\n\"\n",
    "        table += \"<table>\\n\"\n",
    "        table += \"<tr><th>Confusion matrix </th><th>Evaluation measures</th></tr>\\n\"\n",
    "        table += \"<tr><td>\\n\\n\"\n",
    "\n",
    "        table += \"| |\"\n",
    "        for c in self.category_names:\n",
    "            table += c + \"|\"\n",
    "        table += \"\\n|\"\n",
    "        for i in range(len(self.category_names) + 1):\n",
    "            table += \":-:|\"    \n",
    "        table += \"\\n\"\n",
    "        for i, c in enumerate(self.category_names):\n",
    "            table += \"|**\" + c + \"**|\"\n",
    "            for j, _ in enumerate(self.category_names):\n",
    "                table += str(confusion_matrix[i, j]) + \"|\"\n",
    "            table += \"\\n\"\n",
    "        table += \"\\n\"\n",
    "\n",
    "        table += \"</td><td>\\n\\n\"\n",
    "\n",
    "        table += \"| |\"\n",
    "        for c in self.category_names:\n",
    "            table += c + \"|\"\n",
    "        table += \"\\n|\"\n",
    "        for i in range(len(self.category_names) + 1):\n",
    "            table += \":-:|\"    \n",
    "        table += \"\\n\"\n",
    "        table += \"|**Recall**|\"\n",
    "        for i in range(len(self.category_names)):\n",
    "            table += str(\"{:.2f}\".format(self.calculate_recall(confusion_matrix, i))) + \"|\"\n",
    "        table += \"\\n\"\n",
    "        table += \"|**Precision**|\"\n",
    "        for i in range(len(self.category_names)):\n",
    "            table += str(\"{:.2f}\".format(self.calculate_precision(confusion_matrix, i))) + \"|\"\n",
    "        table += \"\\n\\n\"\n",
    "\n",
    "        table += \"</td></tr> </table></center>\\n\"\n",
    "        \n",
    "        display(Markdown(table))\n",
    "    \n",
    "    def show_validation_result(self, field = 'description'):\n",
    "        real_lables = self.validation_data['category'].tolist()\n",
    "        predicted_labels = self.find_categories(self.validation_data ,field)\n",
    "        result = {c : {cc : 0 for cc in self.category_names} for c in self.category_names}\n",
    "        for i in range(len(predicted_labels)):\n",
    "            result[real_lables[i]][predicted_labels[i]] += 1\n",
    "        \n",
    "        confusion_matrix = self.calculate_confusion_matrix(result, self.category_names)\n",
    "        self.show_confusion_matrix_and_evaluation_measures_table(confusion_matrix)\n",
    "        print(\"Accuracy is: {:.2f}\".format(self.calculate_accuracy(confusion_matrix)))\n",
    "        \n",
    "    def evaluate_test(self, filename):\n",
    "        data = pd.read_csv(filename)\n",
    "        data = data.replace(np.nan, '', regex=True)\n",
    "        data = pd.DataFrame(data = {'description' : data.loc[:, 'short_description'] + \" \" + data.loc[:, 'headline']})\n",
    "        data = self.clean_data(data, 'description', self.method)\n",
    "    \n",
    "        return self.find_categories(data, field='description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix and Evaluation Measures\n",
    "\n",
    "### 1.Confusion Matrix\n",
    ">A confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class. In other words, CM \\[i, j\\] shows the number of data which belongs to i category but the model classifies it into j category.\n",
    "### 2. Accuracy\n",
    ">Accuracy shows the fraction of data that classifies correctly.\n",
    "$$ Accuracy = \\frac{Correct \\ Detected}{Total} $$\n",
    "### 3. Recall\n",
    ">If the model can detect the class, the detection is highly trustable.\n",
    "$$ Recall = \\frac{Correct \\ Detected \\ category}{Detected \\ category} $$\n",
    "### 4. Precision\n",
    ">High precision means that the class is detected well.\n",
    "$$ Precision = \\frac{Correct \\ Detected \\ category}{All \\ category} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "In the following parts we show the results of both phase 1 and phase 2 with *lemmatize* method.\n",
    "### Two classes classification using lemmatize and oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<center>\n",
       "<table>\n",
       "<tr><th>Confusion matrix </th><th>Evaluation measures</th></tr>\n",
       "<tr><td>\n",
       "\n",
       "| |BUSINESS|TRAVEL|\n",
       "|:-:|:-:|:-:|\n",
       "|**BUSINESS**|1016|53|\n",
       "|**TRAVEL**|96|1684|\n",
       "\n",
       "</td><td>\n",
       "\n",
       "| |BUSINESS|TRAVEL|\n",
       "|:-:|:-:|:-:|\n",
       "|**Recall**|0.95|0.95|\n",
       "|**Precision**|0.91|0.97|\n",
       "\n",
       "</td></tr> </table></center>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.95\n"
     ]
    }
   ],
   "source": [
    "BC1 = Bayesian_Classifier(method=\"lemmatize\", categories=['BUSINESS', 'TRAVEL'])\n",
    "BC1.show_validation_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three classes classification using lemmatize and oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<center>\n",
       "<table>\n",
       "<tr><th>Confusion matrix </th><th>Evaluation measures</th></tr>\n",
       "<tr><td>\n",
       "\n",
       "| |BUSINESS|STYLE & BEAUTY|TRAVEL|\n",
       "|:-:|:-:|:-:|:-:|\n",
       "|**BUSINESS**|1008|22|39|\n",
       "|**STYLE & BEAUTY**|57|1651|29|\n",
       "|**TRAVEL**|85|63|1632|\n",
       "\n",
       "</td><td>\n",
       "\n",
       "| |BUSINESS|STYLE & BEAUTY|TRAVEL|\n",
       "|:-:|:-:|:-:|:-:|\n",
       "|**Recall**|0.94|0.95|0.92|\n",
       "|**Precision**|0.88|0.95|0.96|\n",
       "\n",
       "</td></tr> </table></center>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.94\n"
     ]
    }
   ],
   "source": [
    "BC2 = Bayesian_Classifier(method=\"lemmatize\")\n",
    "BC2.show_validation_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two classes classification using lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<center>\n",
       "<table>\n",
       "<tr><th>Confusion matrix </th><th>Evaluation measures</th></tr>\n",
       "<tr><td>\n",
       "\n",
       "| |BUSINESS|TRAVEL|\n",
       "|:-:|:-:|:-:|\n",
       "|**BUSINESS**|849|220|\n",
       "|**TRAVEL**|18|1762|\n",
       "\n",
       "</td><td>\n",
       "\n",
       "| |BUSINESS|TRAVEL|\n",
       "|:-:|:-:|:-:|\n",
       "|**Recall**|0.79|0.99|\n",
       "|**Precision**|0.98|0.89|\n",
       "\n",
       "</td></tr> </table></center>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.92\n"
     ]
    }
   ],
   "source": [
    "BC3 = Bayesian_Classifier(method=\"lemmatize\", categories=['BUSINESS', 'TRAVEL'], oversample = False)\n",
    "BC3.show_validation_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three classes classification using lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<center>\n",
       "<table>\n",
       "<tr><th>Confusion matrix </th><th>Evaluation measures</th></tr>\n",
       "<tr><td>\n",
       "\n",
       "| |BUSINESS|STYLE & BEAUTY|TRAVEL|\n",
       "|:-:|:-:|:-:|:-:|\n",
       "|**BUSINESS**|828|73|168|\n",
       "|**STYLE & BEAUTY**|11|1665|61|\n",
       "|**TRAVEL**|10|50|1720|\n",
       "\n",
       "</td><td>\n",
       "\n",
       "| |BUSINESS|STYLE & BEAUTY|TRAVEL|\n",
       "|:-:|:-:|:-:|:-:|\n",
       "|**Recall**|0.77|0.96|0.97|\n",
       "|**Precision**|0.98|0.93|0.88|\n",
       "\n",
       "</td></tr> </table></center>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.92\n"
     ]
    }
   ],
   "source": [
    "BC4 = Bayesian_Classifier(method=\"lemmatize\", oversample = False)\n",
    "BC4.show_validation_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare test labels\n",
    "Using best bayesian classifier, the predicted labels for test data is saved in `output.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = BC2.evaluate_test(\"test.csv\")\n",
    "answer = pd.DataFrame(list(zip([i for i in range(len(labels))], labels)), columns =['index', 'category'])\n",
    "answer.to_csv ('output.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "### 1. Lemmatization vs Stemming\n",
    ">#### Stemming\n",
    "Stemmingalgorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word.\n",
    ">#### Lemmatization\n",
    "Lemmatization, on the other hand, takes into consideration the morphological analysis of the words.\n",
    "\n",
    "It has been seen the benefits of a lemmatizer for search engines, beacuase lemmatization, unlike stemming, reduces the inflected words properly ensuring that the root word belongs to the language\n",
    "\n",
    "### 2. TF-IDF\n",
    "TF-IDF stands for “Term Frequency — Inverse Document Frequency”.\n",
    ">**Term Frequency** measures the frequency of a word in a document. To neutralize the effect of the length of a document, we perform normalization on the frequency value. we divide the frequency with the total number of words in the document.\n",
    "\n",
    "$$ tf(w, d) = \\frac{count(w, d)}{\\sum_{w_i \\in d} count(w_i)}$$\n",
    "\n",
    ">**Document Frequency** measures the importance of document in whole set of corpus, this is very similar to TF. The only difference is that TF is frequency counter for a term t in document d, where as DF is the count of occurrences of term t in the document set N. In other words, DF is the number of documents in which the word is present. We consider one occurrence if the term consists in the document at least once, we do not need to know the number of times the term is present.\n",
    "\n",
    "$$df(w) = occurrence \\ of \\ w \\ in \\ documents$$\n",
    "\n",
    "To train the Bayesian model with TF-IDF, the likelihood would be calculated with TF-IDF value.\n",
    "\n",
    "$$P(w_i|c) = \\frac{tf(w_i, d)\\times\\frac{1}{df(w_i)} + 1}{\\sum_{w \\in V} tf(w_i, d)\\times\\frac{1}{df(w_i)} + |V|}$$\n",
    "\n",
    "### 3. High Precision\n",
    "Precision and recall are two parameter that should be increased together. High precision means that if a classifier detect a class, it would be likely correct. In other words, high precision and low recall result in that the model can’t detect the class well but is highly trustable when it does.\n",
    "\n",
    "### 4. Rare words\n",
    "If there is a rare word \"Tabriz\" in test files that exist in just one category in training data, using a simple posibility calculation may leads to wrong classification, but following equation is used to prevent this issue.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
